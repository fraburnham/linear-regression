<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ReplState" timestamp="1417827710086">{:repl-history {:ide [], :local [&quot;(height-test)&quot; &quot;;above using pmap&quot; &quot;;hypothesis requires the first feature to be 1 always\n(defn hypothesis [thetas features]\n  ;(reduce + (pmap * thetas features)))\n  (reduce + (map * thetas features)))\n\n(defn costfn [hypo-y actual-y]\n  (let [m (count hypo-y)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-y actual-y)))))\n\n;thetas ((t1 t2 t3) (t1 t2 t3))\n;features ((f1 f2 f3) (f1 f2 f3))\n;hypo-y (1 2)\n;actual-y (4 6)\n(defn batch-gradient-descent [thetas alpha features hypo-ys actual-ys]\n  (let [malpha (* alpha (/ 1 (count hypo-ys)))\n        ;diffs (pmap #(- %1 %2) hypo-ys actual-ys)\n        diffs (map #(- %1 %2) hypo-ys actual-ys)\n        ;sums (reduce #(pmap + %1 %2) (map #(map (partial * %1) %2) diffs features))]\n        sums (reduce #(map + %1 %2) (map #(map (partial * %1) %2) diffs features))]\n    ;(pmap (fn [tj sum] (- tj (* malpha sum))) thetas sums)))\n    (map (fn [tj sum] (- tj (* malpha sum))) thetas sums)))\n\n;training-inputs\n;((1 feature feature feature) (1 feature feature feature))\n;thetas\n;(theta0 theta1 theta2)\n(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [;hypo-ys (pmap (partial hypothesis thetas) training-inputs)\n          hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)]\n      (if (or (Double/isNaN (last thetas))\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (map * thetas features)))\n\n(defn costfn [hypo-y actual-y]\n  (let [m (count hypo-y)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-y actual-y)))))\n\n;thetas ((t1 t2 t3) (t1 t2 t3))\n;features ((f1 f2 f3) (f1 f2 f3))\n;hypo-y (1 2)\n;actual-y (4 6)\n(defn batch-gradient-descent [thetas alpha features hypo-ys actual-ys]\n  (let [malpha (* alpha (/ 1 (count hypo-ys)))\n        diffs (map #(- %1 %2) hypo-ys actual-ys)\n        sums (reduce #(map + %1 %2) (map #(map (partial * %1) %2) diffs features))]\n    (map (fn [tj sum] (- tj (* malpha sum))) thetas sums)))\n\n;training-inputs\n;((1 feature feature feature) (1 feature feature feature))\n;thetas\n;(theta0 theta1 theta2)\n(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)]\n      (if (or (Double/isNaN (last thetas))\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(def hypothesis-m (memoize hypothesis))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (map * thetas features)))\n(def hypothesis-m (memoize hypothesis))\n\n(defn costfn [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-ys actual-ys)))))\n(def costfn-m (memoize costfn))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn-m hypo-ys training-outputs)\n          new-cost (costfn-m (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      ;(if (or (Double/isNaN (last thetas))\n      ;        (= thetas new-thetas))\n      (if (&gt; new-cost cost) ;we jumped over the minimum and started going up, break here\n        ;perhaps work on auto adjusting the alpha and starting right before\n        ;the cost went up\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn-m hypo-ys training-outputs)\n          new-cost (costfn-m (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      ;(if (or (Double/isNaN (last thetas))\n      ;        (= thetas new-thetas))\n      (if (&gt; new-cost cost) ;we jumped over the minimum and started going up, break here\n        ;perhaps work on auto adjusting the alpha and starting right before\n        ;the cost went up\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn-m hypo-ys training-outputs)\n          new-cost (costfn-m (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      ;(if (or (Double/isNaN (last thetas))\n      ;        (= thetas new-thetas))\n      (println cost new-cost)\n      (if (&gt; new-cost cost) ;we jumped over the minimum and started going up, break here\n        ;perhaps work on auto adjusting the alpha and starting right before\n        ;the cost went up\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn-m hypo-ys training-outputs)\n          new-cost (costfn-m (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      ;(if (or (Double/isNaN (last thetas))\n      ;        (= thetas new-thetas))\n      (if (or (&gt; new-cost cost)\n              (= new-cost cost))\n        ;we jumped over the minimum and started going up, break here\n        ;perhaps work on auto adjusting the alpha and starting right before\n        ;the cost went up\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn-m hypo-ys training-outputs)\n          new-cost (costfn-m (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      ;(if (or (Double/isNaN (last thetas))\n      ;        (= thetas new-thetas))\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= new-cost cost))\n        ;we jumped over the minimum and started going up, break here\n        ;perhaps work on auto adjusting the alpha and starting right before\n        ;the cost went up\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      ;(if (or (Double/isNaN (last thetas))\n      ;        (= thetas new-thetas))\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= new-cost cost))\n        ;we jumped over the minimum and started going up, break here\n        ;perhaps work on auto adjusting the alpha and starting right before\n        ;the cost went up\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      ;implement costfn for convergence checks.\n      ;if costfn goes up then you've failed\n      (println cost new-cost thetas new-thetas)\n      (if (or (&gt; new-cost cost)\n              (= new-cost cost))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn square [x]\n  (bigdec (* x x)))&quot; &quot;(defn square [x]\n  (rationalize (* x x)))&quot; &quot;(defn square [x]\n  (* x x))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis-m thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn-m hypo-ys training-outputs)\n          new-cost (costfn-m (map (partial hypothesis-m thetas) training-inputs) training-outputs)]\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(deftest univar-height-test\n  (testing \&quot;Testing age vs height regression\&quot;\n    (let [training-inputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2x.dat\&quot;) #\&quot;\\n\&quot;))\n          training-outputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;))\n          [theta0 theta1] (univar-linear-regression 0.07 [0 0]\n                                                    training-inputs\n                                                    training-outputs)\n          y1 (hundredths (univar-hypothesis theta0 theta1 3.5))\n          y2 (hundredths (univar-hypothesis theta0 theta1 7))]\n      (is (= y1 (float 0.97)))\n      (is (= y2 (float 1.2))))))&quot; &quot;(time (univar-height-test))&quot; &quot;0.18 0.89 109.85\n1.0 0.26 155.72\n0.92 0.11 137.66\n0.07 0.37 76.17\n0.85 0.16 139.75\n0.99 0.41 162.6\n0.87 0.47 151.77&quot; &quot;(def t-feats [[1 0.18 0.89] [1 1 0.26] [1 0.92 0.11]\n              [1 0.07 0.37] [1 0.85 0.16] [1 0.99 0.41]\n              [1 0.87 0.47]])&quot; &quot;(def t-out [109.85 155.72 137.66 76.17 139.75 162.6 151.77])&quot; &quot;(hypothesis thetas [1 0.49 0.18])&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis thetas) training-inputs) training-outputs)]\n      (if (&gt;= new-cost cost)\n        #_(= thetas new-thetas)\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis thetas) training-inputs) training-outputs)]\n      (if (&gt;= new-cost cost)\n        #_(= thetas new-thetas)\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(linear-regression 0.07 t-feats t-out)&quot; &quot;(hy)&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis thetas) training-inputs) training-outputs)]\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis thetas) training-inputs) training-outputs)]\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis new-thetas) training-inputs) training-outputs)]\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis thetas) training-inputs) training-outputs)]\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;thetas&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis thetas) training-inputs) training-outputs)]\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)\n          new-cost (costfn (map (partial hypothesis new-thetas) training-inputs) training-outputs)]\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(def thetas (linear-regression 0.003 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.001 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.0003 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.0001 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.00003 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.00001 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.000001 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.0000001 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.0000003 t-feats t-out))&quot; &quot;(def thetas (linear-regression 0.000000003 t-feats t-out))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)]\n      (println cost)\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          cost (costfn hypo-ys training-outputs)]\n      (println cost)\n      (if (or #_(&gt; new-cost cost)\n            (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(&gt; 0.001000462044246042\n   0.0010013506079990244)&quot; &quot;(&lt; 0.001000462044246042\n   0.0010013506079990244)&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (println cost new-cost)\n      (if (or (&gt; new-cost cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 1000]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (println cost new-cost thetas new-thetas)\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(&gt; 5.571182067569726E-7 6.167813473820704E-7)&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (println cost new-cost thetas new-thetas)\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(linear-regression 0.03 t-feats t-out)&quot; &quot;(def thetas (linear-regression 0.03 t-feats t-out))&quot; &quot;(hypothesis thetas [1 0.57 0.83])&quot; &quot;(cons '(1 2 3) '())&quot; &quot;(cons '(1 3 2) (cons '(1 2 3) '()))&quot; &quot;(read-line)&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (pmap (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost) ;this seems backward... old cost should be bigger than new cost...\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))\n&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost) ;this seems backward... old cost should be bigger than new cost...\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (pmap * thetas features)))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (map * thetas features)))&quot; &quot;(time (linear-regression 0.03 t-feats t-out))&quot; &quot;(time (linear-regression 0.003 t-feats t-out))&quot; &quot;(time (linear-regression 0.01 t-feats t-out))&quot; &quot;(time (linear-regression 0.1 t-feats t-out))&quot; &quot;(def hypo-ys (repeat 1000000 123))&quot; &quot;(def actual-ys (repeat 1000000))&quot; &quot;(def actual-ys (repeat 1000000 234))&quot; &quot;(defn costfn-t [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce +\n                         (loop [r (transient [])\n                                hys hypo-ys\n                                ays actual-ys]\n                           (let [diff (- (first hys) (first ays))]\n                             (conj! r (* diff diff))))))))&quot; &quot;(defn costfn-t [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce +\n                         (loop [r (transient [])\n                                hys hypo-ys\n                                ays actual-ys]\n                           (if (empty? hys)\n                             (persistent! r)\n                             (let [diff (- (first hys) (first ays))]\n                               (conj! r (* diff diff)))))))))&quot; &quot;(defn costfn-t [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce +\n                         (loop [r (transient '())\n                                hys hypo-ys\n                                ays actual-ys]\n                           (if (empty? hys)\n                             (persistent! r)\n                             (let [diff (- (first hys) (first ays))]\n                               (conj! r (* diff diff)))))))))&quot; &quot;(transient [a b c])&quot; &quot;(transient ['a 'b 'c])&quot; &quot;(persistent! (transient ['a 'b 'c]))&quot; &quot;(persistent! (transient [2 3 3]))&quot; &quot;(reduce + (persistent! (transient [2 3 3])))&quot; &quot;(defn costfn-t [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce +\n                         (loop [r (transient [])\n                                hys hypo-ys\n                                ays actual-ys]\n                           (if (empty? hys)\n                             (persistent! r)\n                             (let [diff (- (first hys) (first ays))]\n                               (recur (conj! r (* diff diff)) (rest hys) (rest ays)))))))))&quot; &quot;(defn square [x]\n  (* x x))\n\n(defn squared-diff [x y]\n  (square (- x y)))&quot; &quot;(defn costfn [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-ys actual-ys)))))&quot; &quot;(def actual-ys (repeat 1000000 523))&quot; &quot;(def hypo-ys (repeat 1000000 233))&quot; &quot;(set! *warn-on-reflection* true)&quot; &quot;(time (costfn hypo-ys actual-ys))&quot; &quot;(time (costfn-t hypo-ys actual-ys))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (map * thetas features)))\n\n;gonna try to get some speed out of costfn...\n(defn costfn [hypo-ys actual-ys]\n  (let [m (count hypo-ys)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-ys actual-ys)))))\n\n;thetas ((t1 t2 t3) (t1 t2 t3))\n;features ((f1 f2 f3) (f1 f2 f3))\n;hypo-y (1 2)\n;actual-y (4 6)\n(defn batch-gradient-descent [thetas alpha features hypo-ys actual-ys]\n  (let [malpha (* alpha (/ 1 (count hypo-ys)))\n        diffs (map #(- %1 %2) hypo-ys actual-ys)\n        sums (reduce #(map + %1 %2) (map #(map (partial * %1) %2) diffs features))]\n    (map (fn [tj sum] (- tj (* malpha sum))) thetas sums)))\n\n;training-inputs\n;((1 feature feature feature) (1 feature feature feature))\n;thetas\n;(theta0 theta1 theta2)\n(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas (long new-cost))))))&quot; &quot;(require clojure.test)&quot; &quot;(require 'clojure.test)&quot; &quot;(require '[clojure.test :refer :all])&quot; &quot;(defmacro hundredths [x]\n  `(float (/ (Math/round (* ~x 100)) 100)))&quot; &quot;(deftest height-test\n         (testing \&quot;Testing age vs height regression\&quot;\n                  (let [training-inputs (map (partial cons 1)\n                                             (partition 1 (map #(Double/valueOf %)\n                                                               (clojure.string/split\n                                                                 (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2x.dat\&quot;)\n                                                                 #\&quot;\\n\&quot;))))\n                        training-outputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;))\n                        thetas (linear-regression 0.04\n                                                  training-inputs\n                                                  training-outputs)\n                        y1 (hundredths (hypothesis thetas [1 3.5]))\n                        y2 (hundredths (hypothesis thetas [1 7]))]\n                    (is (= y1 (float 0.97)))\n                    (is (= y2 (float 1.2))))))&quot; &quot;(time (height-test))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost (long 0)]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (long (costfn hypo-ys training-outputs))]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          ^long new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          ^Number new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         ^Number cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          ^Number new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         ^Float cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (dec (count (first training-inputs))) (constantly 0)))\n         cost 0]\n    (let [hypo-ys (map (partial hypothesis thetas) training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)\n          new-cost (costfn hypo-ys training-outputs)]\n      (if (or (&gt; cost new-cost)\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas new-cost)))))&quot; &quot;\n(time (height-test))&quot;], :remote []}}</component>
</project>

