<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ReplState" timestamp="1416928441732">{:repl-history {:ide [], :local [&quot;(= (Float/NaN\n     ) (Float/NaN))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (map * thetas features)))\n\n(defn costfn [hypo-y actual-y]\n  (let [m (count hypo-y)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-y actual-y)))))\n\n;thetas ((t1 t2 t3) (t1 t2 t3))\n;features ((f1 f2 f3) (f1 f2 f3))\n;hypo-y (1 2)\n;actual-y (4 6)\n(defn batch-gradient-descent [thetas alpha features hypo-ys actual-ys]\n  (let [malpha (* alpha (/ 1 (count hypo-ys)))]\n    (map (fn [thetas feats]\n           (map (fn [tj fj hypo-y actual-y]\n                  (let [sumsqdiff\n                        (reduce + (map squared-diff hypo-y actual-y))]\n                    (- tj (* malpha (* fj sumsqdiff)))))\n                thetas feats hypo-ys actual-ys))\n         thetas features)))\n\n(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (count training-inputs) (constantly 0)))]\n    (let [hypo-ys (map hypothesis thetas training-inputs)\n          new-thetas (batch-gradien-descent thetas alpha\n                                            training-inputs hypo-ys training-outputs)]\n      ;kay, we got the new-thetas and all that biz, check them for isNaN?\n      (if (or (Double/isNaN (last thetas))\n              (= thetas new-thetas))\n        thetas\n        (recur newthetas)))))&quot; &quot;(defn square [x]\n  (* x x))\n\n(defn squared-diff [x y]\n  (square (- x y)))&quot; &quot;(defn hypothesis [thetas features]\n  (reduce + (map * thetas features)))\n\n(defn costfn [hypo-y actual-y]\n  (let [m (count hypo-y)]\n    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-y actual-y)))))\n\n;thetas ((t1 t2 t3) (t1 t2 t3))\n;features ((f1 f2 f3) (f1 f2 f3))\n;hypo-y (1 2)\n;actual-y (4 6)\n(defn batch-gradient-descent [thetas alpha features hypo-ys actual-ys]\n  (let [malpha (* alpha (/ 1 (count hypo-ys)))]\n    (map (fn [thetas feats]\n           (map (fn [tj fj hypo-y actual-y]\n                  (let [sumsqdiff\n                        (reduce + (map squared-diff hypo-y actual-y))]\n                    (- tj (* malpha (* fj sumsqdiff)))))\n                thetas feats hypo-ys actual-ys))\n         thetas features)))\n\n(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (count training-inputs) (constantly 0)))]\n    (let [hypo-ys (map hypothesis thetas training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)]\n      ;kay, we got the new-thetas and all that biz, check them for isNaN?\n      (if (or (Double/isNaN (last thetas))\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(deftest univar-height-test\n         (testing \&quot;Testing age vs height regression\&quot;\n                  (let [training-inputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2x.dat\&quot;) #\&quot;\\n\&quot;))\n                        training-outputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;))\n                        thetas (linear-regression 0.07\n                                                  training-inputs\n                                                  training-outputs)\n                        y1 (hundredths (hypothesis thetas 3.5))\n                        y2 (hundredths (hypothesis thetas 7))]\n                    (is (= y1 (float 0.97)))\n                    (is (= y2 (float 1.2))))))&quot; &quot;(ns linear-regression.core-test\n  (:require [clojure.test :refer :all]\n            [linear-regression.core :refer :all]))&quot; &quot;(defmacro hundredths [x]\n  `(float (/ (Math/round (* ~x 100)) 100)))&quot; &quot;(deftest height-test\n  (testing \&quot;Testing age vs height regression\&quot;\n    (let [training-inputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2x.dat\&quot;) #\&quot;\\n\&quot;))\n          training-outputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;))\n          thetas (linear-regression 0.07\n                                    training-inputs\n                                    training-outputs)\n          y1 (hundredths (hypothesis thetas 3.5))\n          y2 (hundredths (hypothesis thetas 7))]\n      (is (= y1 (float 0.97)))\n      (is (= y2 (float 1.2))))))&quot; &quot;(height-test)&quot; &quot;(def training-inputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2x.dat\&quot;) #\&quot;\\n\&quot;)))&quot; &quot;(def training-outputs (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;))\n  )&quot; &quot;(defn linear-regression [alpha training-inputs training-outputs]\n  (loop [thetas\n         (cons 1 (repeatedly (count training-inputs) (constantly 0)))]\n    (let [hypo-ys (map hypothesis thetas training-inputs)\n          new-thetas (batch-gradient-descent thetas alpha\n                                             training-inputs hypo-ys training-outputs)]\n      (println thetas)\n      ;kay, we got the new-thetas and all that biz, check them for isNaN?\n      (if (or (Double/isNaN (last thetas))\n              (= thetas new-thetas))\n        thetas\n        (recur new-thetas)))))&quot; &quot;(count training-inputs)&quot; &quot;(repeatedly (count training-inputs) (constantly 0))&quot; &quot;(const 1 (repeatedly (count training-inputs) (constantly 0)))&quot; &quot;(cons 1 (repeatedly (count training-inputs) (constantly 0)))&quot; &quot;(def thetas )&quot; &quot;(def thetas (cons 1 (repeatedly (count training-inputs) (constantly 0))))&quot; &quot;(map hypothesis thetas training-inputs\n     )&quot; &quot;(println thetas\n         )&quot; &quot;(println training-inputs)&quot; &quot;(map * thetas features)&quot; &quot;(map * thetas training-inputs)&quot; &quot;(reduce + (map * thetas training-inputs))&quot; &quot;(def training-inputs (partition 1 (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2x.dat\&quot;) #\&quot;\\n\&quot;))))&quot; &quot;training-inputs&quot; &quot;(map hypothesis thetas (map (comp cons 1) training-inputs))&quot; &quot;(map (comp cons 1) training-inputs)&quot; &quot;(map #(conj % 1) training-inputs)&quot; &quot;(map (comp hypothesis thetas) (map #(conj % 1) training-inputs))&quot; &quot;(hypothesis '(1 0) '(1 2))&quot; &quot;(map println (map #(conj % 1)))&quot; &quot;(map println (map #(conj % 1) training-inputs))&quot; &quot;(def thetas (cons 1 (repeatedly (count (first training-inputs)) (constantly 0))))&quot; &quot;((comp hypothesis thetas) (first training-inputs))&quot; &quot;(comp hypothesis thetas)&quot; &quot;((comp hypothesis thetas) training-inputs)&quot; &quot;(map (partial hypothesis thetas) (map #(conj % 1) training-inputs))&quot; &quot;(def training-outputs (partition 1 (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;)))\n  )&quot; &quot;training-outputs&quot; &quot;(linear-regression 0.07 training-inputs training-outputs)&quot; &quot;(def hypo-ys (map (partial hypothesis thetas) (map #(conj % 1) training-inputs)))&quot; &quot;(def features training-inputs)&quot; &quot;(map (map (fn [tj fj] (println tj fj)) thetas feats) features)&quot; &quot;(map #(map (fn [tj fj] (println tj fj)) thetas %) features)&quot; &quot;(map println features)&quot; &quot;(map #(map println %) features)&quot; &quot;(map #(map println (type %)) features)&quot; &quot;(map #(map (fn [x] (println (type x))) %) features)&quot; &quot;(def features (map #(conj % 1) training-inputs))&quot; &quot;features&quot; &quot;thetas&quot; &quot;hypo-ys&quot; &quot;(def actual-y training-outputs)&quot; &quot;(def actual-ys training-outputs)&quot; &quot;(def actual-ys (map #(Double/valueOf %) (clojure.string/split (slurp \&quot;/home/seditiosus/clojure/linear-regression/ex2y.dat\&quot;) #\&quot;\\n\&quot;)))&quot; &quot;actual-ys&quot; &quot;(- (first hypo-ys) (first actual-ys))&quot; &quot;(* (- (first hypo-ys) (first actual-ys)) (first features))&quot; &quot;(* (- (first hypo-ys) (first actual-ys)) (first (first features)))&quot; &quot;(map (fn [feats] (map (fn [hypo-y actual-y fj]\n                        (* (- hypo-y actual-y) fj))) features))&quot; &quot;(map (fn [feats] (map (fn [hypo-y actual-y fj]\n                        (* (- hypo-y actual-y) fj)) hypo-ys actual-ys) features))&quot; &quot;(map (fn [feats] (map (fn [hypo-y actual-y fj]\n                        (* (- hypo-y actual-y) fj)) hypo-ys actual-ys)) features)&quot; &quot;(map (fn [feats] (map (fn [hypo-y actual-y fj]\n                        (* (- hypo-y actual-y) fj)) hypo-ys actual-ys feats)) features)&quot; &quot;(map (fn [hypo-y actual-y fj]\n       (* (- hypo-y actual-y) fj)) hypo-ys actual-ys (first features))&quot; &quot;(map (fn [hypo-y actual-y fj]\n       (println (* (- hypo-y actual-y) fj))\n       (* (- hypo-y actual-y) fj)) hypo-ys actual-ys (first features))&quot; &quot;(map (fn [hypo-y actual-y fj]\n       (println fj)\n       (* (- hypo-y actual-y) fj)) hypo-ys actual-ys (first (first features)))&quot; &quot;(map (fn [hypo-y actual-y fj]\n       (println fj)\n       (* (- hypo-y actual-y) fj)) hypo-ys actual-ys (first features))&quot; &quot;(map (fn [hypo-y actual-y] (- hypo-y actual-y)) hypo-ys actual-ys)&quot; &quot;(defn diffs (map (fn [hypo-y actual-y] (- hypo-y actual-y)) hypo-ys actual-ys))&quot; &quot;(def diffs (map (fn [hypo-y actual-y] (- hypo-y actual-y)) hypo-ys actual-ys))&quot; &quot;(first (first features))&quot; &quot;(map println (first features))&quot; &quot;(map (partial * (first (first features))) diffs)&quot; &quot;(map #(map (partial * %) diffs) (first features)\n     )&quot; &quot;(map #(map (partial * %) diffs) (first features))&quot; &quot;(map (fn [feats] (map #(map (partial * %) diffs) feats)) features)&quot; &quot;(def diffs-times-featj (map (fn [feats] (map #(map (partial * %) diffs) feats)) features))&quot; &quot;(count diffs-times-featj)&quot; &quot;(first diffs-times-featj)&quot; &quot;(first (first diffs-times-featj))&quot; &quot;(count (first (first diffs-times-featj)))&quot; &quot;(first thetas)&quot; &quot;(reduce + (first (first diffs-times-featj)))&quot; &quot;(def malpha (* 0.07 (/ 1 50)))&quot; &quot;malpha&quot; &quot;(* malpha (reduce + (first (first diffs-times-featj)))\n   )&quot; &quot;(- (first theta) (* malpha (reduce + (first (first diffs-times-featj)))))&quot; &quot;(- (first thetas) (* malpha (reduce + (first (first diffs-times-featj)))))&quot; &quot;diffs&quot; &quot;(map (fn [diff]\n       (* diff (first (first features)))) diffs)&quot; &quot;(reduce + (map (fn [diff]\n       (* diff (first (first features)))) diffs))&quot; &quot;(println \&quot;wazzup\&quot;)&quot; &quot;(map (fn [feats] (map #(reduce + (map (partial * %) diffs)) feats)) features)&quot; &quot;(def items [\&quot;a\&quot; \&quot;b\&quot;])&quot; &quot;(map (fn [item] \n       [\&quot;number\&quot; item]) items)&quot; &quot;(cons [\&quot;date\&quot; \&quot;Date\&quot;] (map (fn [item] \n       [\&quot;number\&quot; item]) items))&quot; &quot;(into [] (cons [\&quot;date\&quot; \&quot;Date\&quot;] (map (fn [item] \n       [\&quot;number\&quot; item]) items)))&quot; &quot;(+ (* 80 15) (* 40 25)) &quot; &quot;(sort '(2 4 3 1))&quot;], :remote []}}</component>
</project>

