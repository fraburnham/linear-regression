(ns linear-regression.core)

;*
; Squares x
; @param x A number
; @return x squared
;*
(defn square [x]
  (* x x))

;*
; Returns the squared difference between x and y
; @param x A number
; @param y A number
; @return squared difference of x and y
;*
(defn squared-diff [x y]
  (square (- x y)))

;*
; Performs the hypothesis for linear regression (sum of theta*feature)
; @param thetas Is usually generated by the program. (t1 t2 t3)
; @param features the input features (f1 f2 f3) the first feature must always
;                 be 1
; @return the estimate of y based on thetas and features
;*
(defn linear-hypothesis [thetas features]
  (reduce + (map * thetas features)))

;*
; Determines the cost of the thetas used to generate the hypothetical outputs.
; @param hypo-ys outputs from calling linear-hypothesis over all the training
;                features
; @param actual-ys expected outputs from training data
; @return the cost of the thetas used to generate hypo-ys
;*
(defn linear-costfn [hypo-ys actual-ys]
  (let [m (count hypo-ys)]
    (/ 1 (* 2 m) (reduce + (map squared-diff hypo-ys actual-ys)))))

;*
; Performs the hypothesis for logistic regression
; @param thetas Is usually generated by the program. (t1 t2 t3)
; @param features the input features (f1 f2 f3) the first feature must always
;                 be 1
; @return the estimate of y based on thetas and features
;*
(defn logistic-hypothesis [thetas features]
  (/ 1 (+ 1 (* (Math/E) (- (reduce + (map * thetas features)))))))

;*
; Determines the cost of the thetas used to generate the hypothetical outputs.
; @param hypo-ys outputs from calling linear-hypothesis over all the training
;                features
; @param actual-ys expected outputs from training data
; @return the cost of the thetas used to generate hypo-ys
;*
(defn logistic-costfn [hypo-ys actual-ys]
  (map (fn [hypo-y actual-y]
         (if (= actual-y 1)
           (- (Math/log hypo-y))
           (- (Math/log (- 1 hypo-y)))))
       hypo-ys actual-ys))

;*
; Performs gradient descent to adjust thetas to their optimal values.
; @param thetas theta values in the format (t1 t2 t3)
; @param alpha the learning rate
; @param features training features in the format ((1 f1 f2 f3) (1 f1 f2 f3))
; @param hypo-ys the predicted outputs (h-y1 h-y2 h-y3)
; @param actual-ys the actual training outputs (a-y1 a-y2 a-y3)
; @return updated theta values
;*
(defn batch-gradient-descent [thetas alpha features hypo-ys actual-ys]
  (let [malpha (* alpha (/ 1 (count hypo-ys)))
        diffs (map #(- %1 %2) hypo-ys actual-ys)
        sums (reduce #(map + %1 %2)
                     (map #(map (partial * %1) %2) diffs features))]
    (map (fn [tj sum] (- tj (* malpha sum))) thetas sums)))

;*
; Perform regression (logistic or linear based on the hypothesis-fn) using
; training-inputs and training-outputs.
; @param hypothesis-fn function used to predict an output for input
;                      and thetas
; @param cost-fn function used to determine the cost of a set of thetas
; @param alpha learning rate
; @param training-inputs inputs to train the regression algorithm
; @param training-outputs outputs to train the regression algorithm
; @return final thetas that can be passed, as is, to the provided hypothesis
;         functions
;*
(defn regression [hypothesis-fn cost-fn alpha training-inputs training-outputs]
  (loop [thetas
         (cons 1 (repeatedly (dec (count (first training-inputs)))
                             (constantly 0)))
         cost 0]
    (let [hypo-ys (map (partial hypothesis-fn thetas) training-inputs)
          new-thetas (batch-gradient-descent thetas alpha
                                             training-inputs hypo-ys
                                             training-outputs)
          new-cost (cost-fn hypo-ys training-outputs)]
      (if (or (> cost new-cost)
              (= thetas new-thetas))
        thetas
        (recur new-thetas new-cost)))))
